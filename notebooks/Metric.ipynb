{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4066bf44-335b-4816-9bd7-ef34a664e3c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision.transforms as transforms\n",
    "from sklearn import linear_model, model_selection\n",
    "import os\n",
    "\n",
    "from src.train_cifar_checkpoints import load_cifar10, evaluate\n",
    "from src.resnet import resnet18\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2de2602-c305-4e50-b19a-9026e8b386a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterate_checkpoints(root, num_checkpoints, prefix=\"retain\"):\n",
    "    \"\"\"\n",
    "    Iterate on checkpoints in directory, if not all checkpoints are available, \n",
    "    \n",
    "    \"\"\"\n",
    "    model = resnet18()\n",
    "    model.linear = nn.Linear(512, 10)\n",
    "    model.to(DEVICE)\n",
    "\n",
    "    previous_model = None\n",
    "\n",
    "    for i in range(num_checkpoints):\n",
    "        fname = os.path.join(root, f\"{prefix}_{i}.pt\")\n",
    "        if os.path.isfile(fname):\n",
    "            model.load_state_dict(torch.load(fname))\n",
    "            previous_model = copy.deepcopy(model)\n",
    "            yield model\n",
    "        else:\n",
    "            break\n",
    "\n",
    "def compute_function_out(out, y, mode=\"loss\"):\n",
    "    if mode == \"loss\":\n",
    "        return F.cross_entropy(out, y, reduction=\"none\")\n",
    "    elif mode == \"confidence\":\n",
    "        return out[torch.arange(len(out)).to(out.device), y]\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def collect_losses(loader, root, root2=None, num_models=100, mode=\"loss\"):\n",
    "    \"\"\" \n",
    "    Collect all losses from both model trained from scratch on retain and unlearned models\n",
    "    \"\"\"\n",
    "    losses = []\n",
    "    labels = []\n",
    "    labels_member = []\n",
    "    correct = []\n",
    "\n",
    "    if root2 is None:\n",
    "        root2 = root\n",
    "    \n",
    "    for x, y in loader:\n",
    "        x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "\n",
    "        batch_losses = []\n",
    "        batch_labels = []\n",
    "        batch_labels_member = []\n",
    "        batch_correct = []\n",
    "        \n",
    "        # Retain losses\n",
    "        for model in iterate_checkpoints(root, num_models, prefix=\"retain\"):\n",
    "            out = model(x)\n",
    "            pred = out.argmax(1)\n",
    "\n",
    "            loss = compute_function_out(out, y, mode)\n",
    "\n",
    "            batch_losses.append(loss.cpu())\n",
    "            batch_labels.append(y.cpu())\n",
    "            batch_labels_member.append(torch.zeros_like(loss).cpu())\n",
    "            batch_correct.append((pred == y).float())\n",
    "            \n",
    "        # Unlearned losses\n",
    "        for model in iterate_checkpoints(root2, num_models, prefix=\"unlearn\"):\n",
    "            out = model(x)\n",
    "            pred = out.argmax(1)\n",
    "            \n",
    "            loss = compute_function_out(out, y, mode)\n",
    "            \n",
    "            batch_losses.append(loss.cpu())\n",
    "            batch_labels.append(y.cpu())\n",
    "            batch_labels_member.append(torch.ones_like(loss).cpu())\n",
    "            batch_correct.append((pred == y).float())\n",
    "\n",
    "\n",
    "        losses.append(torch.stack(batch_losses))\n",
    "        labels.append(torch.stack(batch_labels))\n",
    "        labels_member.append(torch.stack(batch_labels_member))\n",
    "        correct.append(torch.stack(batch_correct))\n",
    "\n",
    "\n",
    "    return torch.cat(losses, dim=1).t(), torch.cat(labels, dim=1).t(), torch.cat(labels_member, dim=1).t(), torch.cat(correct, dim=1).t()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179ad52e-dacf-48b7-b50c-0cc715529c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = load_cifar10(use_transforms=False)\n",
    "\n",
    "\n",
    "model = resnet18()\n",
    "model.linear = nn.Linear(512, 10)\n",
    "model.to(DEVICE)\n",
    "\n",
    "model.load_state_dict(torch.load(\"../cifar10_checkpoints/initial.pt\"))\n",
    "\n",
    "indices_forget = np.load(\"../cifar10_checkpoints/forget_set.npy\")\n",
    "\n",
    "forget_set = torch.utils.data.Subset(train, indices_forget)\n",
    "\n",
    "forget_loader = torch.utils.data.DataLoader(forget_set, batch_size=64)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(test, batch_size=64)\n",
    "\n",
    "losses, labels, labels_member, correct = collect_losses(forget_loader, \"../cifar10_checkpoints/\", root2=\"../unlearning_checkpoints/\", num_models=100, mode=\"loss\")\n",
    "\n",
    "# losses = [num_examples, num_models*2]\n",
    "# It will contain the individual \"forget\" sample losses for all unlearned models and models trained from scratch on retain set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10fdf2af-e298-4e35-b11e-d88c276cfbe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "index = pd.MultiIndex.from_product([range(losses.shape[0]), range(losses.shape[1])], names=['Sample', 'Prediction'])\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame({'Loss': losses.numpy().flatten(), 'Labels': labels.cpu().numpy().flatten(), 'Member_Label': labels_member.cpu().numpy().flatten(), 'Correct': correct.cpu().numpy().flatten()}, index=index)\n",
    "df = df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f785d92e-8b78-4533-ad50-03d4f8ee83a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This shows a per-sample loss distribution, separating in member / non-member\n",
    "sample_index = 30\n",
    "\n",
    "sns.kdeplot(data=df[df.Sample==sample_index], x=\"Loss\", hue=\"Member_Label\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d639f9",
   "metadata": {},
   "source": [
    "# Metric reproduction\n",
    "\n",
    "The below part attempts to reproduce the metric described in the challenge describtion pdf \"Evaluation for the NeurIPS Machine Unlearning Competition\"\n",
    "\n",
    "Some details about this particular implementation:\n",
    "\n",
    "- In the pdf, it is mentionned that several different attacks are run on a single sample, here only one attack is run per sample (subject to change)\n",
    "- Since the exact quantity used for the computation of the metric is not revealed, we propose here to use either the loss or the \"confidence level\" as the quantity to compute the score. This leads to scores that cannot be compared directly with the online scores, but that attempt to give a better idea of the success chance of a method that is tested locally with this score\n",
    "- Additionnally to the score, we report the average accuracy of attacks. In general, the score should increase when the accuracy decreases\n",
    "- We skip the part where the score is shrinked in case the model performs poorly on test set, this loss of performance can be estimated by looking at the test accuracy of the checkpoints and comparing it to the initial checkpoint performance.\n",
    "- We also provide below a different kind of \"simpler\" attack, which has been provided in the starting kit at \"https://github.com/unlearning-challenge/starting-kit/blob/main/unlearning-CIFAR10.ipynb\" It is nice to look at both metrics, in general, a good method should have both score but also provide a low accuracy for this attack. The problem with this simple attack is that it is quite \"weak\" and thus does not vary enough when exposed to different unlearning method (typically, it will give 55% when not using unlearning, and 51% when using a lot of different unlearning methods that can have very different scores).\n",
    "- There might be some warning inside the score computation, they happen when the variability of unlearned models output is too low (this will translate into a \"spike\" for most forget samples in the cell above). However, the computation of the score should take that into account and will give low score to these samples for which the variability is too low."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07304283-7cdc-4e63-8557-d2bf29d2fec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import make_scorer, confusion_matrix, accuracy_score\n",
    "\n",
    "delta = 1e-6\n",
    "\n",
    "# This is the maximum epsilon we can observe, according to the pdf\n",
    "# it depends on the number of model considered (for 512 it is 6.5)\n",
    "# Here it corresponds to 100 checkpoints, you need to change it if you consider\n",
    "# more or less checkpoints\n",
    "\n",
    "epsilon_threshold = 3.89\n",
    "\n",
    "def bucket_score(scalar):\n",
    "    if np.isinf(scalar):\n",
    "        return np.nan\n",
    "\n",
    "    # Should tune this value anytime we change N\n",
    "    if scalar > epsilon_threshold:\n",
    "        return 2/ 2 ** (int(epsilon_threshold / 0.5) + 1)\n",
    "        \n",
    "    return 2 / 2 **(int(scalar / 0.5) + 1)\n",
    "\n",
    "def custom_scorer(y, y_pred):\n",
    "    tn, fp, fn, tp = confusion_matrix(y, y_pred).ravel()\n",
    "    fpr = fp / (fp + tn)\n",
    "    fnr = fn / (fn + tp)\n",
    "\n",
    "    if fpr == 0 and fnr == 0:\n",
    "        return bucket_score(np.inf)\n",
    "\n",
    "    if fpr == 0 or fnr == 0:\n",
    "        return np.nan\n",
    "\n",
    "    e1 = np.log(1 - delta - fpr) - np.log(fnr)\n",
    "    e2 = np.log(1 - delta - fnr) - np.log(fpr)\n",
    "    return bucket_score(np.nanmax([e1, e2]))#bucket_score(np.nanmax([e1, e2]))\n",
    "\n",
    "# Also can use \"accuracy\" as a simpler scorer\n",
    "\n",
    "scores = []\n",
    "accuracy_scores = []\n",
    "\n",
    "for sample_idx in range(len(losses)):\n",
    "    loss_list = losses[sample_idx, :]\n",
    "    label_list = labels_member[sample_idx, :]\n",
    "\n",
    "    attack = linear_model.LogisticRegression()\n",
    "\n",
    "    score = np.nanmean(model_selection.cross_val_score(attack, loss_list.reshape(len(loss_list), 1), label_list, cv=2, scoring=make_scorer(custom_scorer)))\n",
    "    acc_score = model_selection.cross_val_score(attack, loss_list.reshape(len(loss_list), 1), label_list, cv=2, scoring=\"accuracy\").mean()\n",
    "    \n",
    "    scores.append(score)\n",
    "    accuracy_scores.append(acc_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be703f02-04ce-4b5c-ba5b-141d776afabb",
   "metadata": {},
   "outputs": [],
   "source": [
    "average_score = np.nanmean(scores)\n",
    "average_acc_score = np.mean(accuracy_scores)\n",
    "print(f\"Average Score: {average_score}, Average attack accuracy: {average_acc_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2596931b-672c-4c29-8d8d-2f002020a31e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple attack stats\n",
    "\n",
    "@torch.no_grad()\n",
    "def collect_losses_simple(loader, model, device=\"cuda\"):\n",
    "    model.eval()\n",
    "    all_losses = []\n",
    "    for x, y in loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        out = model(x)\n",
    "        losses = F.cross_entropy(out, y, reduction=\"none\")\n",
    "        all_losses.append(losses)\n",
    "    return torch.cat(all_losses)\n",
    "\n",
    "\n",
    "def simple_mia(sample_loss, members, n_splits=10, random_state=0):\n",
    "    \"\"\"Computes cross-validation score of a membership inference attack.\n",
    "\n",
    "\n",
    "    Args:\n",
    "      sample_loss : array_like of shape (n,).\n",
    "        objective function evaluated on n samples.\n",
    "      members : array_like of shape (n,),\n",
    "        whether a sample was used for training.\n",
    "      n_splits: int\n",
    "        number of splits to use in the cross-validation.\n",
    "    Returns:\n",
    "      scores : array_like of size (n_splits,)\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    attack_model = linear_model.LogisticRegression()\n",
    "    cv = model_selection.StratifiedShuffleSplit(\n",
    "        n_splits=n_splits, random_state=random_state\n",
    "    )\n",
    "    return model_selection.cross_val_score(\n",
    "        attack_model, sample_loss, members, cv=cv, scoring=\"accuracy\"\n",
    "    )\n",
    "\n",
    "\n",
    "def run_attack(forget_loader, test_loader, model_to_test):\n",
    "    ft_forget_losses = collect_losses_simple(forget_loader, model_to_test).cpu().numpy()\n",
    "    ft_test_losses = collect_losses_simple(test_loader, model_to_test).cpu().numpy()\n",
    "    \n",
    "    # Subsampling to have class balanced (member, non member)\n",
    "    \n",
    "    if len(ft_forget_losses) > len(ft_test_losses):\n",
    "        np.random.shuffle(ft_forget_losses)\n",
    "        ft_forget_losses = ft_forget_losses[:len(ft_test_losses)]\n",
    "    else:\n",
    "        np.random.shuffle(ft_test_losses)\n",
    "        ft_test_losses = ft_test_losses[:len(ft_forget_losses)]\n",
    "    \n",
    "    samples_mia_ft = np.concatenate((ft_test_losses, ft_forget_losses)).reshape((-1, 1))\n",
    "    labels_mia = [0] * len(ft_test_losses) + [1] * len(ft_forget_losses)\n",
    "    \n",
    "    mia_scores_ft = simple_mia(samples_mia_ft, labels_mia)\n",
    "    \n",
    "    print(\n",
    "        f\"The MIA attack has an accuracy of {mia_scores_ft.mean():.3f} on forgotten vs unseen images\"\n",
    "    )\n",
    "    return mia_scores_ft.mean()\n",
    "    \n",
    "\n",
    "def run_multiple_attacks(forget_loader, test_loader):\n",
    "    accuracies = []\n",
    "    for model in iterate_checkpoints(\"../unlearning_checkpoints/\", 100, prefix=\"unlearn\"):\n",
    "        acc = run_attack(forget_loader, test_loader, model)\n",
    "        accuracies.append(acc)\n",
    "    return np.mean(accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b3b51fd-606b-4fa5-b04e-fdd2fa982fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We average the simple attack score over all of the unlearned checkpoints\n",
    "\n",
    "average_simple_acc = run_multiple_attacks(forget_loader, test_loader)\n",
    "print(average_simple_acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
