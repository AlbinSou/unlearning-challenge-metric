{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b06fb389",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision.transforms as transforms\n",
    "from sklearn import linear_model, model_selection\n",
    "\n",
    "from PyTorch_CIFAR10.cifar10_models.resnet import resnet18\n",
    "\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c01c6019",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_cifar10():\n",
    "    default_cifar10_train_transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(\n",
    "            (0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)\n",
    "        ),\n",
    "    ]\n",
    "    )\n",
    "    default_cifar10_eval_transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(\n",
    "            (0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)\n",
    "        ),\n",
    "    ]\n",
    "    )\n",
    "    \n",
    "    train = torchvision.datasets.CIFAR10(root=\"/DATA/data\", train=True, transform=default_cifar10_train_transform)\n",
    "    test = torchvision.datasets.CIFAR10(root=\"/DATA/data\", train=False, transform=default_cifar10_eval_transform)\n",
    "    return train, test\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(loader, model):\n",
    "    model.eval()\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    for x, y in loader:\n",
    "        x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "        out = model(x)\n",
    "        pred = out.argmax(1)\n",
    "        total += len(x)\n",
    "        correct += (pred == y).float().sum()\n",
    "    \n",
    "    return correct / total\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "391b39d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def finetune(\n",
    "    net, \n",
    "    loader, \n",
    "    epochs=10,\n",
    "    weight_decay=5e-4,\n",
    "    lr=0.001,\n",
    "    momentum=0.,\n",
    "    use_scheduler=True,\n",
    "    ):\n",
    "    \"\"\"Simple unlearning by finetuning.\"\"\"\n",
    "    total_iters = epochs * len(loader)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(net.parameters(), lr=lr,\n",
    "                      momentum=momentum, weight_decay=weight_decay)\n",
    "    \n",
    "    if use_scheduler:\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=total_iters)\n",
    "    net.train()\n",
    "\n",
    "    for ep in range(epochs):\n",
    "        net.train()\n",
    "        for inputs, targets in loader:\n",
    "            inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)\n",
    "        \n",
    "            optimizer.zero_grad()\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if use_scheduler:\n",
    "                scheduler.step()\n",
    "        \n",
    "    net.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e3af18f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model arithmetic\n",
    "\n",
    "def get_flat(model):\n",
    "    return torch.cat([p.view(-1) for p in model.state_dict().values()])\n",
    "\n",
    "def from_flat(flat_params, model, in_place=False):\n",
    "    # Loads params from flat vector\n",
    "    index = 0\n",
    "    \n",
    "    if not in_place:\n",
    "        model = copy.deepcopy(model)\n",
    "        \n",
    "    sd = model.state_dict()\n",
    "    for k, p in sd.items():\n",
    "        prodshape = torch.prod(torch.tensor(p.shape))\n",
    "        if prodshape == 1:\n",
    "            sd[k] = flat_params[index].type(p.data.type()).view(p.shape)\n",
    "        else:\n",
    "            sd[k] = flat_params[index : index + prodshape].view(*p.shape)\n",
    "        index += int(prodshape)\n",
    "    model.load_state_dict(sd)\n",
    "    return model\n",
    "\n",
    "def add_random_noise(model, strength=0.2):\n",
    "    noise_model = copy.deepcopy(model)\n",
    "    for name, module in noise_model.named_modules():\n",
    "        if hasattr(module, \"reset_parameters\"):\n",
    "            module.reset_parameters()\n",
    "            \n",
    "    for param, noise in zip(model.parameters(), noise_model.parameters()):\n",
    "        param.data += strength * noise\n",
    "\n",
    "\n",
    "def model_op(net1, net2, operator=lambda x, y: x + y):\n",
    "    return_model = copy.deepcopy(net1)\n",
    "    state_dict = {}\n",
    "    for (n1, p1), (n2, p2) in zip(net1.state_dict().items(), net2.state_dict().items()):\n",
    "        state_dict[n1] = operator(p1, p2)\n",
    "    return_model.load_state_dict(state_dict)\n",
    "    return return_model\n",
    "\n",
    "def arithmetic_unlearning(model, model_ft_forget, power=0.1):\n",
    "    flat_1, flat_2 = get_flat(model), get_flat(model_ft_forget)\n",
    "    task_vector = flat_2 - flat_1\n",
    "    #task_vector = model_op(model, model_ft_forget, lambda x, y: y - x)\n",
    "    #unlearned_model = model_op(model, task_vector, lambda x, y: x - power*y)\n",
    "    unlearned_model = from_flat(flat_1 - power * task_vector, model)\n",
    "    return unlearned_model\n",
    "\n",
    "def arithmetic_unlearning_2(model, model_ft_forget, model_ft_retain, power=0.1):\n",
    "    flat_1, flat_2, flat_3 = get_flat(model), get_flat(model_ft_forget), get_flat(model_ft_retain)\n",
    "    \n",
    "    vect_retain = flat_3 - flat_1\n",
    "    vect_forget = flat_2 - flat_1\n",
    "    \n",
    "    po_vr_vf = (torch.dot(vect_retain, vect_forget) / torch.dot(vect_forget, vect_forget)) * vect_forget\n",
    "    \n",
    "    return from_flat(flat_1 + power*(vect_retain - po_vr_vf), model)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def adapt_bs(model, loader, num_epochs=2):\n",
    "    model.train()\n",
    "    for i in range(num_epochs):\n",
    "        for x, y in loader:\n",
    "            x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "            model(x)\n",
    "    model.eval()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4765928",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "\n",
    "@torch.no_grad()\n",
    "def collect_losses(loader, model, device=\"cuda\"):\n",
    "    model.eval()\n",
    "    all_losses = []\n",
    "    for x, y in loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        out = model(x)\n",
    "        losses = F.cross_entropy(out, y, reduction=\"none\")\n",
    "        all_losses.append(losses)\n",
    "    return torch.cat(all_losses)\n",
    "        \n",
    "\n",
    "def simple_mia(sample_loss, members, n_splits=10, random_state=0):\n",
    "    \"\"\"Computes cross-validation score of a membership inference attack.\n",
    "\n",
    "    Args:\n",
    "      sample_loss : array_like of shape (n,).\n",
    "        objective function evaluated on n samples.\n",
    "      members : array_like of shape (n,),\n",
    "        whether a sample was used for training.\n",
    "      n_splits: int\n",
    "        number of splits to use in the cross-validation.\n",
    "    Returns:\n",
    "      scores : array_like of size (n_splits,)\n",
    "    \"\"\"\n",
    "\n",
    "    attack_model = linear_model.LogisticRegression()\n",
    "    cv = model_selection.StratifiedShuffleSplit(\n",
    "        n_splits=n_splits, random_state=random_state\n",
    "    )\n",
    "    return model_selection.cross_val_score(\n",
    "        attack_model, sample_loss, members, cv=cv, scoring=\"accuracy\"\n",
    "    )\n",
    "\n",
    "def run_attack(forget_loader, test_loader, model_to_test):\n",
    "\n",
    "    ft_forget_losses = collect_losses(forget_loader, model_to_test).cpu().numpy()\n",
    "    ft_test_losses = collect_losses(test_loader, model_to_test).cpu().numpy()\n",
    "    \n",
    "    # Subsampling to have class balanced (member, non member)\n",
    "    \n",
    "    if len(ft_forget_losses) > len(ft_test_losses):\n",
    "        np.random.shuffle(ft_forget_losses)\n",
    "        ft_forget_losses = ft_forget_losses[:len(ft_test_losses)]\n",
    "    else:\n",
    "        np.random.shuffle(ft_test_losses)\n",
    "        ft_test_losses = ft_test_losses[:len(ft_forget_losses)]\n",
    "    \n",
    "    samples_mia_ft = np.concatenate((ft_test_losses, ft_forget_losses)).reshape((-1, 1))\n",
    "    labels_mia = [0] * len(ft_test_losses) + [1] * len(ft_forget_losses)\n",
    "    \n",
    "    mia_scores_ft = simple_mia(samples_mia_ft, labels_mia)\n",
    "    \n",
    "    print(\n",
    "        f\"The MIA attack has an accuracy of {mia_scores_ft.mean():.3f} on forgotten vs unseen images\"\n",
    "    )\n",
    "    \n",
    "    return mia_scores_ft.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9531bc1c-4096-4f06-9d1d-fb1455c63416",
   "metadata": {},
   "outputs": [],
   "source": [
    "def finetune_project(\n",
    "    net,\n",
    "    retain_loader,\n",
    "    vect_forget,\n",
    "    epochs=10,\n",
    "    weight_decay=5e-4,\n",
    "    lr=0.001,\n",
    "    momentum=0.,\n",
    "    use_scheduler=True,\n",
    "    ):\n",
    "\n",
    "    total_iter_number = len(retain_loader) * epochs\n",
    "\n",
    "    new_net = copy.deepcopy(net)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(new_net.parameters(), lr=lr,\n",
    "                      momentum=momentum, weight_decay=weight_decay)\n",
    "    \n",
    "    if use_scheduler:\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=total_iter_number)\n",
    "\n",
    "    flat_initial = get_flat(net)\n",
    "\n",
    "    new_net.train()\n",
    "\n",
    "    for ep in range(epochs):\n",
    "        new_net.train()\n",
    "        for inputs, targets in retain_loader:\n",
    "            inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)\n",
    "        \n",
    "            optimizer.zero_grad()\n",
    "            outputs = new_net(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Remove forget component of the update\n",
    "            flat_current = get_flat(new_net) - flat_initial\n",
    "            po_vr_vf = (torch.dot(flat_current, vect_forget) / torch.dot(vect_forget, vect_forget)) * vect_forget\n",
    "\n",
    "            flat_current = flat_initial + flat_current - po_vr_vf\n",
    "            from_flat(flat_current, new_net, in_place=True)\n",
    "\n",
    "        \n",
    "            if use_scheduler:\n",
    "                scheduler.step()\n",
    "        \n",
    "    new_net.eval()\n",
    "    return new_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4542d3f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model and compute acc on test set\n",
    "\n",
    "model = resnet18(pretrained=True)\n",
    "model = model.to(DEVICE)\n",
    "train, test = load_cifar10()\n",
    "\n",
    "shuffled_indices = torch.randperm(len(train))\n",
    "\n",
    "LEN_FORGET = 2000\n",
    "LEN_RETAIN = len(train) - LEN_FORGET\n",
    "\n",
    "forget_set = torch.utils.data.Subset(train, shuffled_indices[:LEN_FORGET])\n",
    "retain_set = torch.utils.data.Subset(train, shuffled_indices[LEN_FORGET:])\n",
    "\n",
    "forget_loader = torch.utils.data.DataLoader(forget_set, batch_size=64, shuffle=True)\n",
    "retain_loader = torch.utils.data.DataLoader(retain_set, batch_size=64, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test, batch_size=64, shuffle=True)\n",
    "\n",
    "run_attack(forget_loader, test_loader, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7448902b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 1: Finetuning on retain set\n",
    "\n",
    "model_unlearned = copy.deepcopy(model)\n",
    "add_random_noise(model_unlearned, strength=0.05)\n",
    "finetune(model_unlearned, retain_loader, epochs=1, momentum=0.9, lr=0.001)\n",
    "\n",
    "run_attack(forget_loader, test_loader, model_unlearned)\n",
    "acc = evaluate(test_loader, model_unlearned)\n",
    "print(\"Test Accuracy\", float(acc.cpu()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aaad1ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 2: Finetune on forget set and retire lambda * forget_set_task_vector\n",
    "\n",
    "model_ft_forget = copy.deepcopy(model)\n",
    "finetune(model_ft_forget, forget_loader, epochs=50)\n",
    "\n",
    "for power in np.linspace(0, 1.0, 20):\n",
    "    print(power)\n",
    "    model_unlearned = arithmetic_unlearning(model, model_ft_forget, power)\n",
    "    run_attack(forget_loader, test_loader, model_unlearned)\n",
    "    acc = evaluate(test_loader, model_unlearned)\n",
    "    print(\"Test Accuracy\", float(acc.cpu()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a49c248",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 3: Finetune on both retain and forget and move in direction of model + retain_vector - proj(retain_vector, forget_vector)\n",
    "\n",
    "model_ft_forget = copy.deepcopy(model)\n",
    "finetune(model_ft_forget, forget_loader, epochs=50)\n",
    "\n",
    "model_ft_retain = copy.deepcopy(model)\n",
    "finetune(model_ft_retain, retain_loader, epochs=10)\n",
    "\n",
    "for power in np.linspace(0, 0.5, 20):\n",
    "    print(power)\n",
    "    model_unlearned = arithmetic_unlearning_2(model, model_ft_forget, model_ft_retain, power)\n",
    "    run_attack(forget_loader, test_loader, model_unlearned)\n",
    "    acc = evaluate(test_loader, model_unlearned)\n",
    "    print(\"Test Accuracy\", float(acc.cpu()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e449b0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check linear connectivity\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "accuracies = []\n",
    "\n",
    "model_ft_retain = copy.deepcopy(model)\n",
    "finetune(model_ft_retain, retain_loader, epochs=10)\n",
    "\n",
    "flat_1 = get_flat(model)\n",
    "flat_2 = get_flat(model_ft_retain)\n",
    "vect = flat_2 - flat_1\n",
    "\n",
    "powers = np.linspace(0, 1., 20)\n",
    "\n",
    "for power in powers:\n",
    "    model_inter = from_flat(flat_1 + power*vect, model)\n",
    "    #adapt_bs(model_inter, forget_loader)\n",
    "    acc = evaluate(test_loader, model_inter)\n",
    "    accuracies.append(float(acc.cpu()))\n",
    "    \n",
    "plt.plot(powers, accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a44bfca",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(retain_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0366a12e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 4: Finetune on forget with an additional logit and change forget set target to that logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2785dae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 5 Finetune on retain but always remove forget component from the update (project on subspace orthogonal to forget vector at every update)\n",
    "\n",
    "model_ft_forget = copy.deepcopy(model)\n",
    "finetune(model_ft_forget, forget_loader, epochs=10, momentum=0.)\n",
    "\n",
    "vect_forget = get_flat(model_ft_forget) - get_flat(model)\n",
    "\n",
    "model_unlearned = finetune_project(\n",
    "    model,\n",
    "    retain_loader,\n",
    "    vect_forget,\n",
    "    epochs=2,\n",
    "    weight_decay=5e-4,\n",
    "    lr=0.001,\n",
    "    momentum=0.9,\n",
    "    use_scheduler=True,\n",
    "    )\n",
    "\n",
    "run_attack(forget_loader, test_loader, model_unlearned)\n",
    "acc = evaluate(test_loader, model_unlearned)\n",
    "print(\"Test Accuracy: \", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "360d0b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_model = evaluate(test_loader, model, device=DEVICE)\n",
    "acc_unlearned = evaluate(test_loader, model_unlearned, device=DEVICE)\n",
    "\n",
    "print(f\"Accuracy of base model: {acc_model}\")\n",
    "print(f\"Accuracy of unlearned model: {acc_unlearned}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
